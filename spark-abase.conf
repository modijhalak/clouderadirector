# DBS Cloudera Director Bootstrap Template
# comments
# - This template is used to setup HBase on-prem cluster
name: "#[[CLUSTER_NAME]]"
provider {
    type: "byon"
    hosts: "#[[CLUSTER_HOSTS]]"
}
ssh {
    username: "#[[SUDO_USER]]"
    privateKey: "#[[KEYFILE]]"
}
common-instance-properties {
    image: "NA"
    tags {
        App-code: "ADA"
    }
}
instances {
    on-premise-cm: ${common-instance-properties} {
        type: "on-premise-cm"
        preferredHosts: "#[[MANAGER_HOST]]"
    }
    on-premise: ${common-instance-properties} {
        type: "on-premise"
        bootstrapScripts: [ """#!/bin/sh
# This is an embedded bootstrap script that runs as root and can be used to customize
# the instances immediately after boot and before any other Cloudera Director action
# If the exit code is not zero Cloudera Director will automatically retry
touch /cd-bootstrap.txt
echo "Couldera Director bootstrap started ..." >> /cd-bootstrap.txt
  # setup proxy
export no_proxy="#[[NO_PROXY]]"
echo "export no_proxy=\"#[[NO_PROXY]]\"" >> ~/.bashrc
echo "export no_proxy=\"#[[NO_PROXY]]\"" >> /home/#[[SUDO_USER]]/.bashrc
#echo "proxy=http://${proxy_user}:${proxy_pwd}@10.5.127.23:8080" >> /etc/yum.conf
echo "Couldera Director bootstrap finished." >> /cd-bootstrap.txt
exit 0
    """ ]
    }
}
databaseServers {
    existingmysql1 {
        type: mysql
        host: "#[[DB_HOST]]"
        port: 3306
        user: "#[[DB_SUPER_USER]]"
        password: "#[[DB_ROOT_PASSWORD]]"
    }
}
cloudera-manager {
    instance: ${instances.on-premise-cm} {
        tags {
            application: "#[[CLUSTER_NAME]]-CM"
        }
    }
    javaInstallationStrategy: NONE
    enableEnterpriseTrial: "true"
    repository: "#[[LOCAL_YUM_REPO]]/cm5/redhat/7/x86_64/cm/5.12.0/"
    repositoryKeyUrl: "#[[LOCAL_YUM_REPO]]/cm5/redhat/7/x86_64/cm/RPM-GPG-KEY-cloudera"
    krbAdminUsername: "#[[KRB_ADMIN_USER]]"
    krbAdminPassword: "#[[KRB_ADMIN_PWD]]"
    # Custom service descriptors for external parcels
    # Spark 2.2 requires Java 8. Earlier versions of Spark 2 do not work with CDH 5.13
    # https://www.cloudera.com/documentation/director/latest/topics/director_using_spark2.html
    csds: [
       "#[[LOCAL_PARCEL_REPO]]/spark2/csd/SPARK2_ON_YARN-2.2.0.cloudera1.jar"
    ]
    configs {
        CLOUDERA_MANAGER {
             remote_parcel_repo_urls: "#[[LOCAL_PARCEL_REPO]]/cdh5/parcels/5.12/,#[[LOCAL_PARCEL_REPO]]/kafka/parcels/2.2/,#[[LOCAL_PARCEL_REPO]]/accumulo-c5/parcels/1.7.2/,#[[LOCAL_PARCEL_REPO]]/spark2/parcels/2.2.0.cloudera1/"
             enable_api_debug: true
             KDC_TYPE: "#[[KRB_KDC_TYPE]]"
             KDC_HOST: "#[[KRB_KDC_HOST]]"
             SECURITY_REALM: "#[[KRB_SECURITY_REALM]]"
             AD_KDC_DOMAIN: "#[[KRB_AD_KDC_DOMAIN]]"
#             KDC_HOST: "ad-dns-ntp.dahngdp.local"
#             KDC_HOST: "10.115.80.195"
#             SECURITY_REALM: "DAHNGDP.LOCAL"
#             AD_KDC_DOMAIN: "ou=Cloudera,DC=dahngdp,DC=local"
             KRB_MANAGE_KRB5_CONF: "true"
#             KRB_ENC_TYPES: "rc4-hmac aes256-cts des3-hmac-sha1 arcfour-hmac des-hmac-sha1 des-cbc-md5 des-cbc-crc"
#             KRB_ENC_TYPES: "rc4-hmac des3-hmac-sha1 arcfour-hmac des-hmac-sha1 des-cbc-md5 des-cbc-crc"
              KRB_ENC_TYPES: "rc4-hmac aes256-cts"
       }
       SERVICEMONITOR {
             firehose_non_java_memory_bytes: 4500000000
       }
       HOSTMONITOR {
             firehose_non_java_memory_bytes: 4500000000
       }
    }
    databases {
         CLOUDERA_MANAGER {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: "3306"
             user: "scm"
             password: "#[[DB_ROOT_PASSWORD]]"
             name: "scm"
         }
         ACTIVITYMONITOR {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: "3306"
             user: "amon"
             password: "#[[DB_ROOT_PASSWORD]]"
             name: "amon"
         }
         REPORTSMANAGER {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: "3306"
             user: "rman"
             password: "#[[DB_ROOT_PASSWORD]]"
             name: "rman"
         }
        NAVIGATOR {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: "3306"
             user: "nav"
             password: "#[[DB_ROOT_PASSWORD]]"
             name: "nav"
         }
     # Added in Cloudera Manager 5.2+
         NAVIGATORMETASERVER {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: "3306"
             user: "navms"
             password: "#[[DB_ROOT_PASSWORD]]"
             name: "navms"
         }
    }
#    databaseTemplates {
#        CLOUDERA_MANAGER {
#            name: cmtemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: scm
#            usernamePrefix: cmadmin
#        }
#
#        ACTIVITYMONITOR {
#            name: amontemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: amon
#            usernamePrefix: amadmin
#        }
#
#        REPORTSMANAGER {
#            name: rmantemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: rman
#            usernamePrefix: rmadmin
#        }
#
#        NAVIGATOR {
#            name: navtemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: nav
#            usernamePrefix: nadmin
#        }
#
#        # Added in Cloudera Manager 5.2+
#        NAVIGATORMETASERVER {
#            name: navmetatemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: navmeta
#            usernamePrefix: nmadmin
#        }
#    }
}
cluster {
    products {
        CDH: 5.12
#        ACCUMULO: 1.7.2
        SPARK2: 2.2.0.cloudera1
        KAFKA: 2
    }
#    parcelRepositories: ["#[[LOCAL_PARCEL_REPO]]/cdh5/parcels/5.12/",
#        "#[[LOCAL_PARCEL_REPO]]/accumulo-c5/parcels/1.7.2/",
#        "#[[LOCAL_PARCEL_REPO]]/spark2/parcels/2.2.0.cloudera1/",
#        "#[[LOCAL_PARCEL_REPO]]/spark2/parcels/2.2/
#        "#[[LOCAL_PARCEL_REPO]]/kafka/parcels/2.2/"
#    ]
    parcelRepositories: ["#[[LOCAL_PARCEL_REPO]]/cdh5/parcels/5.12/",
        "#[[LOCAL_PARCEL_REPO]]/spark2/parcels/2.2/
        "#[[LOCAL_PARCEL_REPO]]/kafka/parcels/2.2/"
    ]
#    services: [HDFS, ZOOKEEPER, YARN, SPARK_ON_YARN, SPARK2_ON_YARN, ACCUMULO16]
    services: [HDFS, ZOOKEEPER, YARN, SPARK_ON_YARN, SPARK2_ON_YARN, HBASE, HIVE, IMPALA]
    configs {
        # HDFS fencing should be set to true for HA configurations
        HDFS {
            dfs_ha_fencing_methods: "shell(true)"
            dfs_replication: "3"
            dfs_block_local_path_access_user: "impala,hbase,mapred,spark"
        }
        HIVE {
            audit_event_log_dir: /data/log/hive/audit
            lineage_event_log_dir: /data/log/hive/lineage
        }
        HBASE {
            audit_event_log_dir: /data/log/hbase/audit
        }
    }
    
    databases {
         HIVE {
             type: "mysql"
             host: "#[[DB_HOST]]"
             port: 3306
             user: hive
             password: "#[[DB_ROOT_PASSWORD]]"
             name: hive
         }
    }
#    databaseTemplates: {
#
#        HIVE {
#            name: hivetemplate
#            databaseServerName: existingmysql1 # Must correspond to an external database server named above
#            databaseNamePrefix: hivemetastore
#            usernamePrefix: hive
#        }
#
#    }
    masters-1 {
        count: 2
        instance: ${instances.on-premise} {
            tags {
                group: masters-1
            }
        }
        # HIVESERVER2 roles need a SPARK role (such as gateway) on the same
        # instance to pick up Spark configurations
        roles {
            # ZooKeeper uses majority quorum for r/w, configure odd number of servers.
            HDFS: [NAMENODE, FAILOVERCONTROLLER, JOURNALNODE]
            ZOOKEEPER: [SERVER]
            HIVE: [HIVESERVER2, HIVEMETASTORE]
            YARN: [RESOURCEMANAGER]
            SPARK_ON_YARN: [GATEWAY]
            HBASE: [MASTER]
        }
        # NameNode nameservice, autofailover, and quorum journal name must be configured for high availability
        configs {
            HDFS {
                NAMENODE {
                    dfs_federation_namenode_nameservice: hanameservice
                    autofailover_enabled: true
                    dfs_namenode_quorum_journal_name: hanameservice
                    dfs_name_dir_list: /data/nn
                }
            }
        }
    }
    masters-2 {
        count: 1
        instance: ${instances.on-premise} {
            tags {
                group: master
            }
        }
        roles {
            # ZooKeeper uses majority quorum for r/w, configure odd number of servers.
            HDFS: [JOURNALNODE, HTTPFS] # HTTPFS role needed for HUE
            ZOOKEEPER: [SERVER]
            YARN: [JOBHISTORY]
            HBASE: [HBASETHRIFTSERVER] # HBASETHRIFTSERVER role needed for HUE
            SPARK_ON_YARN: [SPARK_YARN_HISTORY_SERVER]
            SPARK2_ON_YARN: [SPARK2_YARN_HISTORY_SERVER]
            IMPALA: [CATALOGSERVER, STATESTORE]
        }
        configs {
            HDFS {
                NAMENODE {
                    dfs_name_dir_list: /data/nn
                }
            }
        }
    }
    workers {
        count: #[[WORKER_HOST_COUNT]]
        minCount: #[[WORKER_HOST_COUNT]]
        instance: ${instances.on-premise} {
            tags {
                group: worker
            }
        }
        roles {
            HDFS: [DATANODE]
            YARN: [NODEMANAGER]
            HBASE: [REGIONSERVER]
            IMPALA: [IMPALAD]
        }
        configs {
            HDFS {
                DATANODE {
                   dfs_data_dir_list: "/data/dn1, /data/dn2, /data/dn3"
                }
            }
        }
    }
    gateways {
        count: #[[GATEWAY_HOST_COUNT]]
        instance: ${instances.on-premise} {
            tags {
                group: gateway
            }
        }
        roles {
            HBASE: [GATEWAY]
            HIVE: [GATEWAY]
            SPARK2_ON_YARN: [GATEWAY]
        }
        # Optional custom role configurations
        # Configuration keys containing periods must be enclosed in double quotes.
        # configs {
        # }
    }
}
